---
title: "Complex Data Exercise"
author: Cassia Roth
date: "`r Sys.Date()`"
---

This exercise is designed to give you practice with exploring text datasets (a _complex data type_). We will load, process, and engage in some discriptive/exploratory analysis of the data.

This dataset, titled `Sherlock`, is a package that includes text from the *Sherlock Holmes* book series by Sir Arthur Conan Doyle. All 48 texts are in the public domain. Information regarding copyright laws is available [here](https://sherlock-holm.es/ascii/).

I found this dataset through [Emil Hvitfeldt](https://github.com/EmilHvitfeldt)'s [R-text-data compilation repository](https://github.com/EmilHvitfeldt/R-text-data.git). I also am using [Paul Vanderlaken's website](https://paulvanderlaken.com/2017/08/03/harry-plotter-celebrating-the-20-year-anniversary-with-tidytext-the-tidyverse-and-r/) as guidance.

First, we will install and load the dataset.
```{r}
#suppressing log messages
#| message: false
#| warning: false
#| include: false

#installing dataset and other needed packages
devtools::install_github("EmilHvitfeldt/sherlock")

#loading dataset and other necessary packages
library(sherlock)
library(tidytext)
library(dplyr)
library(tidyverse)
library(stringr)
library(tibble)
library(knitr)
library(here)
```
Next, we will explore the dataset. We will start by looking at the structure.
```{r, message=FALSE, warning=FALSE}
#Exploring the dataset
dplyr::glimpse(holmes)

#Viewing the first few rows (variable names "text" and "book")
head(holmes)

#Understanding all books in the data
book_titles <- holmes %>% distinct(book)
print(book_titles)

#Ordering the entries alphabetically by book title
book_titles <- book_titles[order(book_titles$book), ]

#Creating a table using kable
table1 <- kable(book_titles, caption = "Book Titles")
print(table1)
```

Now let's see how many times some of the most important people's names appears in the texts.
```{r}
#Searching for specific words
specific_words <- c("sherlock", "holmes", "moriarty", "watson", "john")

#Creating a word frequency table for specific words
word_freq_table <- holmes %>%
  unnest_tokens(word, text) %>%
  filter(word %in% specific_words) %>%
  count(word, sort = TRUE)

word_freq_df <- as.data.frame(word_freq_table)
print(word_freq_df)
```

Mistakenly, many people attribute the quote ["Elementary, my dear Watson,"](https://www.ihearofsherlock.com/2016/08/elementary-my-dear-watson-birth-of.html) to the *Sherlock Holmes* series. So let's check how many times the phrase appears in the data. For fun, we will also see if Sherlock Holmes ever calls his sidekick "John Watson" by his full name.
```{r}
#Searching for phrases about Watson
phrases <- c("John Watson", "Elementary, my dear Watson")

holmes_filtered <- holmes %>%
  filter(str_detect(text, fixed(phrases[1])) | 
         str_detect(text, fixed(phrases[2])))

#Viewing the filtered dataset
print(holmes_filtered)
```
We can see that "John Watson" appears only twice, when he is writing about his own experiences. The phrase "Elementary, my dear Watson," appears *zero* times. So, it's a myth that Sherlock Holmes ever said this phrase in the original texts.

Now let's visualize some of the words in a word cloud using the `wordcloud2` package.

```{r}
# Install and load the wordcloud2 package
# install.packages("wordcloud2")
library(wordcloud2)

# Load stopwords
stop_words <- tidytext::stop_words$word

# Remove stopwords from the text data
holmes_wc <- holmes %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words)

# Create word frequency table
word_freq_table <- holmes_wc %>%
  count(word, sort = TRUE)

# Visualize word cloud
wordcloud2(word_freq_table, size = 1.5)
``````

We can see that the word "Holmes" is the most frequent word in the dataset, appearing 2403 times. This is expected, as the dataset is about Sherlock Holmes. Watson (appearing 809 times) is also quite frequent, as he is the narrator of the stories. other words that stood out to me include "London" (340 times), "police" (283 times), and "Sherlock" (383 times).