[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/cassiaroth/Documents/GitHub/MADARoth/cassiaroth-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Party                 0             1 FALSE          4\n  top_counts                    \n1 M: 4, F: 3, O: 2              \n2 Dem: 5, Lib: 2, Gre: 1, Rep: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  38.9 23.2   9  23  33  56   77 ▅▇▂▂▅\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible.\n#Box plot comparing political party (x) and height (y) using ggplot function geom_boxplot(). The image is named party-height.png\n\np5 &lt;- mydata %&gt;% ggplot(aes(x=Party, y=Height)) + geom_boxplot() \nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"party-height.png\")\nggsave(filename = figure_file, plot= p5) \n\nSaving 7 x 5 in image\n\n\n#Scatterplot with weight (x) versus age (y) using ggplot function geom_point(). The image is named weight-age.png\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y=Age)) + geom_point()\nplot(p6)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-age.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "cdcdata-exercise",
    "section": "",
    "text": "CDC Data Exploration: Quarterly Provisional Estimates for Selected Birth Indicators\n\nDescription of the data\nThis dataset comes from the National Center for Health Statistics (NCHS) and is a part of the CDC’s National Vital Statistics System. The data is a quarterly release of provisional estimates of selected reproductive indicators including general fertility rates, age-specific birth rates, total and low-risk cesarean delivery rates, preterm birth rates, and other gestational age categories.\nThe dataset is available at the CDC website.\n\n\nReading in the data\nHere, I loaded all the packages I will use for this data exercise, listed below. I also read in the data from the csv file, and I called the data cdcdata. We can see that there are 8 variables and 1100 observations. This dataset has multiple indicators stacked on top of each other. For example, within the “Topic” column, there are metrics for “Birth Rate”, “Gestational Age,” etc., each measuring a very different outcome. Instead of widening the dataset (splitting the data into multiple tables, one for each metric), I am going to make things easier by looking at one metric, “Age-specific Birth Rates,” which I will get to below.\n\n#Loading packages\nlibrary(tidyverse) #This includes ggplot2, tidyr, readr, dplyr, stringr, purr, forcats\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(knitr)\nlibrary(here)\n\nhere() starts at /Users/cassiaroth/Documents/GitHub/MADARoth/cassiaroth-MADA-portfolio\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n#Reading in the csv file\ncdcdata &lt;- read_csv(here(\"cdcdata-exercise\", \"NCHS_VSRR_Quarterly_provisional_estimates_for_selected_birth_indicators_20240205.csv\"))\n\nRows: 1100 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): Year and Quarter, Topic, Topic Subgroup, Indicator, Race Ethnicity ...\ndbl (1): Rate\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Checking the packaging (displaying first and last few rows of data)\nnrow(cdcdata)\n\n[1] 1100\n\nncol(cdcdata)\n\n[1] 8\n\n#Showing the structure\nstr(cdcdata)\n\nspc_tbl_ [1,100 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Year and Quarter       : chr [1:1100] \"2023 Q3\" \"2023 Q3\" \"2023 Q3\" \"2023 Q3\" ...\n $ Topic                  : chr [1:1100] \"Birth Rates\" \"Birth Rates\" \"Birth Rates\" \"Birth Rates\" ...\n $ Topic Subgroup         : chr [1:1100] \"Age-specific Birth Rates\" \"Age-specific Birth Rates\" \"Age-specific Birth Rates\" \"Age-specific Birth Rates\" ...\n $ Indicator              : chr [1:1100] \"10-14 years\" \"10-14 years\" \"10-14 years\" \"10-14 years\" ...\n $ Race Ethnicity Category: chr [1:1100] \"All races and origins\" \"Hispanic\" \"Non-Hispanic Black\" \"Non-Hispanic White\" ...\n $ Rate                   : num [1:1100] 0.2 0.3 0.4 0.1 13.4 21.3 19.7 8.7 56 78.3 ...\n $ Unit                   : chr [1:1100] \"per 1,000 population\" \"per 1,000 population\" \"per 1,000 population\" \"per 1,000 population\" ...\n $ Significant            : chr [1:1100] NA NA \"*\" NA ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Year and Quarter` = col_character(),\n  ..   Topic = col_character(),\n  ..   `Topic Subgroup` = col_character(),\n  ..   Indicator = col_character(),\n  ..   `Race Ethnicity Category` = col_character(),\n  ..   Rate = col_double(),\n  ..   Unit = col_character(),\n  ..   Significant = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n#Looking at top/bottom of data\nhead(cdcdata)[, c(1:8)]\n\n# A tibble: 6 × 8\n  `Year and Quarter` Topic     `Topic Subgroup` Indicator Race Ethnicity Categ…¹\n  &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt;                 \n1 2023 Q3            Birth Ra… Age-specific Bi… 10-14 ye… All races and origins \n2 2023 Q3            Birth Ra… Age-specific Bi… 10-14 ye… Hispanic              \n3 2023 Q3            Birth Ra… Age-specific Bi… 10-14 ye… Non-Hispanic Black    \n4 2023 Q3            Birth Ra… Age-specific Bi… 10-14 ye… Non-Hispanic White    \n5 2023 Q3            Birth Ra… Age-specific Bi… 15-19 ye… All races and origins \n6 2023 Q3            Birth Ra… Age-specific Bi… 15-19 ye… Hispanic              \n# ℹ abbreviated name: ¹​`Race Ethnicity Category`\n# ℹ 3 more variables: Rate &lt;dbl&gt;, Unit &lt;chr&gt;, Significant &lt;chr&gt;\n\ntail(cdcdata)[, c(1:8)]\n\n# A tibble: 6 × 8\n  `Year and Quarter` Topic     `Topic Subgroup` Indicator Race Ethnicity Categ…¹\n  &lt;chr&gt;              &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt;                 \n1 2021 Q1            NICU Adm… NICU Admission … NICU Adm… Non-Hispanic Black    \n2 2021 Q1            NICU Adm… NICU Admission … NICU Adm… Non-Hispanic White    \n3 2021 Q1            Source o… Medicaid         Medicaid  All races and origins \n4 2021 Q1            Source o… Medicaid         Medicaid  Hispanic              \n5 2021 Q1            Source o… Medicaid         Medicaid  Non-Hispanic Black    \n6 2021 Q1            Source o… Medicaid         Medicaid  Non-Hispanic White    \n# ℹ abbreviated name: ¹​`Race Ethnicity Category`\n# ℹ 3 more variables: Rate &lt;dbl&gt;, Unit &lt;chr&gt;, Significant &lt;chr&gt;\n\n\nThis dataset is pretty untidy. The “Year and Quarter” column is a combination of the year and the quarter. I will separate this column into two columns, “Year” and “Quarter”, and convert them to numeric. In addition, the “Topic” and “Topic Subgroup” columns have multiple indicators listed in the same column. I will be looking at the observations only for “Birth Rates” and will remove all other observations. I will also replace the space with an underscore in the “Topic” column where it contains “Birth Rates.”\n\n#This section was written with help from ChatGPT.\n\n#Get column names\ncolnames(cdcdata)\n\n[1] \"Year and Quarter\"        \"Topic\"                  \n[3] \"Topic Subgroup\"          \"Indicator\"              \n[5] \"Race Ethnicity Category\" \"Rate\"                   \n[7] \"Unit\"                    \"Significant\"            \n\n#Separate the \"Year and Quarter\" column into \"Year\" and \"Quarter\" columns in cdcdata\ncdcdata &lt;- cdcdata %&gt;%\n  separate(\"Year and Quarter\", into = c(\"Year\", \"Quarter\"), sep = \" Q\")\n\n#Convert \"Year\" and \"Quarter\" columns to numeric\ncdcdata &lt;- cdcdata %&gt;%\n  mutate(\n    Year = as.numeric(Year),\n    Quarter = case_when(\n      Quarter == \"1\" ~ 1,\n      Quarter == \"2\" ~ 2,\n      Quarter == \"3\" ~ 3,\n      Quarter == \"4\" ~ 4\n    )\n  )\n\n#Rename observations \"Birth Rates\"\ncdcdata &lt;- cdcdata %&gt;%\n  mutate(Topic = ifelse(Topic == \"Birth Rates\", str_replace(Topic, \" \", \"_\"), Topic))\n\n# Filter to include only observations where the \"Topic\" column contains \"Birth_Rates\"\ncdcdata1 &lt;- cdcdata %&gt;% filter(Topic == \"Birth_Rates\")\n\n#Print the modified cdcdata dataset\nprint(cdcdata1)\n\n# A tibble: 572 × 9\n    Year Quarter Topic   `Topic Subgroup` Indicator Race Ethnicity Categ…¹  Rate\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt;                  &lt;dbl&gt;\n 1  2023       3 Birth_… Age-specific Bi… 10-14 ye… All races and origins    0.2\n 2  2023       3 Birth_… Age-specific Bi… 10-14 ye… Hispanic                 0.3\n 3  2023       3 Birth_… Age-specific Bi… 10-14 ye… Non-Hispanic Black       0.4\n 4  2023       3 Birth_… Age-specific Bi… 10-14 ye… Non-Hispanic White       0.1\n 5  2023       3 Birth_… Age-specific Bi… 15-19 ye… All races and origins   13.4\n 6  2023       3 Birth_… Age-specific Bi… 15-19 ye… Hispanic                21.3\n 7  2023       3 Birth_… Age-specific Bi… 15-19 ye… Non-Hispanic Black      19.7\n 8  2023       3 Birth_… Age-specific Bi… 15-19 ye… Non-Hispanic White       8.7\n 9  2023       3 Birth_… Age-specific Bi… 20-24 ye… All races and origins   56  \n10  2023       3 Birth_… Age-specific Bi… 20-24 ye… Hispanic                78.3\n# ℹ 562 more rows\n# ℹ abbreviated name: ¹​`Race Ethnicity Category`\n# ℹ 2 more variables: Unit &lt;chr&gt;, Significant &lt;chr&gt;\n\n\nWith our new dataset, called cdcdata_birth_rates, we can see that the “Year and Quarter” column has been separated into “Year” and “Quarter” columns, and the “Topic” column has been modified to replace the space with an underscore where it contains “Birth Rates.” We can also see that the “Topic” column contains only “Birth Rates” observations. This gives us 9 columns (variables) and 572 rows (observations).\nAdditionally, the “Topic” and “Subtopic” columns are redundant in that the “Subtopic” column is the true indicator we are looking for, and we can remove the “Topic” column. The same issue we had in the “Topic” column is also present in the “Topic Subgroup” column. We will remove the “Topic” column and filter the dataset to include only observations where the “Topic Subgroup” column is exactly “Age-specific Birth Rates.” We will also replace “Age-specific Birth Rates” with “Age_Specific” in the “Topic Subgroup” column.\n\n#This section was written with help from ChatGPT.\n\n#Remove the \"Topic\" column\ncdcdata1 &lt;- cdcdata1 %&gt;%\n  select(-Topic)\n\n#Filter the dataset to include only observations where the \"Topic Subgroup\" column is exactly \"Age-specific Birth Rates\"\ncdcdata1 &lt;- cdcdata1 %&gt;%\n  filter(`Topic Subgroup` == \"Age-specific Birth Rates\")\n\n#Replace \"Age-specific Birth Rates\" with \"Age_Specific\" in the \"Topic Subgroup\" column\ncdcdata1 &lt;- cdcdata1 %&gt;%\n  mutate(`Topic Subgroup` = str_replace(`Topic Subgroup`, \"Age-specific Birth Rates\", \"Age_Specific\"))\n\n#Remove the \"Topic Subgroup\" column and rename dataframe\ncdcdata2 &lt;- cdcdata1 %&gt;%\n  select(-`Topic Subgroup`)\n\n#Print the modified cdcdata dataset\nprint(cdcdata2)\n\n# A tibble: 352 × 7\n    Year Quarter Indicator   `Race Ethnicity Category`  Rate Unit    Significant\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;                     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      \n 1  2023       3 10-14 years All races and origins       0.2 per 1,… &lt;NA&gt;       \n 2  2023       3 10-14 years Hispanic                    0.3 per 1,… &lt;NA&gt;       \n 3  2023       3 10-14 years Non-Hispanic Black          0.4 per 1,… *          \n 4  2023       3 10-14 years Non-Hispanic White          0.1 per 1,… &lt;NA&gt;       \n 5  2023       3 15-19 years All races and origins      13.4 per 1,… *          \n 6  2023       3 15-19 years Hispanic                   21.3 per 1,… *          \n 7  2023       3 15-19 years Non-Hispanic Black         19.7 per 1,… *          \n 8  2023       3 15-19 years Non-Hispanic White          8.7 per 1,… *          \n 9  2023       3 20-24 years All races and origins      56   per 1,… *          \n10  2023       3 20-24 years Hispanic                   78.3 per 1,… *          \n# ℹ 342 more rows\n\n\nNow, we have 7 columns (variables) and 352 observations, since we are only looking at age-specific birth rates, we don’t need a column with that information.\nNow let’s clean up the rest of the column names and observation entries, removing spaces and recategorizing variables.\n\n#This section was written with help from ChatGPT.\n\n#Renaming columns\ncdcdata2 &lt;- cdcdata2 %&gt;%\n  rename(Age_Years = Indicator)\n\ncdcdata2 &lt;- cdcdata2 %&gt;%\n  rename(Race_Ethnicity = `Race Ethnicity Category`)\n\n#Define the specific levels for the \"Race_Ethnicity\" category\nrace_ethnicity_levels &lt;- c(\"All races and origins\", \"Hispanic\", \"Non-Hispanic Black\", \"Non-Hispanic White\")\n\n#Convert the \"Race Ethnicity Category\" column to a factor variable with custom levels\ncdcdata2$Race_Ethnicity &lt;- factor(cdcdata2$Race_Ethnicity, levels = race_ethnicity_levels)\n\n#View the levels of the \"Race_Ethnicity\" column\nlevels(cdcdata2$Race_Ethnicity)\n\n[1] \"All races and origins\" \"Hispanic\"              \"Non-Hispanic Black\"   \n[4] \"Non-Hispanic White\"   \n\n#Get unique age category levels from the \"Age_Years\" column\nage_levels &lt;- unique(cdcdata2$Age_Years)\n\n#Convert the \"Age_Years\" column to a factor variable with custom levels\ncdcdata2$Age_Years &lt;- factor(cdcdata2$Age_Years, levels = age_levels)\n\n#View the levels of the \"Age_Years\" column\nlevels(cdcdata2$Age_Years)\n\n[1] \"10-14 years\" \"15-19 years\" \"20-24 years\" \"25-29 years\" \"30-34 years\"\n[6] \"35-39 years\" \"40-44 years\" \"45+ years\"  \n\n#Print the modified cdcdata dataset\nprint(cdcdata2)\n\n# A tibble: 352 × 7\n    Year Quarter Age_Years   Race_Ethnicity         Rate Unit        Significant\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                 &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;      \n 1  2023       3 10-14 years All races and origins   0.2 per 1,000 … &lt;NA&gt;       \n 2  2023       3 10-14 years Hispanic                0.3 per 1,000 … &lt;NA&gt;       \n 3  2023       3 10-14 years Non-Hispanic Black      0.4 per 1,000 … *          \n 4  2023       3 10-14 years Non-Hispanic White      0.1 per 1,000 … &lt;NA&gt;       \n 5  2023       3 15-19 years All races and origins  13.4 per 1,000 … *          \n 6  2023       3 15-19 years Hispanic               21.3 per 1,000 … *          \n 7  2023       3 15-19 years Non-Hispanic Black     19.7 per 1,000 … *          \n 8  2023       3 15-19 years Non-Hispanic White      8.7 per 1,000 … *          \n 9  2023       3 20-24 years All races and origins  56   per 1,000 … *          \n10  2023       3 20-24 years Hispanic               78.3 per 1,000 … *          \n# ℹ 342 more rows\n\n\nWe can see that we have multiple missing entries for the “Significant” Column. From the data dictionary on the website, the CDC defines the “Significant” variable as follows: “An asterisk (*) indicates that estimates for the most recent quarter are significantly different from the same quarter of the previous year.” This variable is not relevant for this analysis, so we can remove this column from the dataset. When CDC calculated rates, they had population size in each group, and thus could calculate significance.\nBut first, let’s just check how many observations are missing an asterisk in the “Significant” column.\n\n# Count the number of NA values in the \"Significant\" column\nna_count &lt;- sum(is.na(cdcdata2$Significant))\n\n# Print the number of NA values\nprint(na_count)\n\n[1] 327\n\n\nWe can see that 327 of the 352 observations were not significantly different from the same quarter of the previous year.\nNow, let’s remove that column/variable altogether.\n\n#Remove the \"Significant\" column\ncdcdata2 &lt;- cdcdata2 %&gt;%\n  select(-Significant)\n\n#Print the new dataset\nprint(cdcdata2)\n\n# A tibble: 352 × 6\n    Year Quarter Age_Years   Race_Ethnicity         Rate Unit                \n   &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;       &lt;fct&gt;                 &lt;dbl&gt; &lt;chr&gt;               \n 1  2023       3 10-14 years All races and origins   0.2 per 1,000 population\n 2  2023       3 10-14 years Hispanic                0.3 per 1,000 population\n 3  2023       3 10-14 years Non-Hispanic Black      0.4 per 1,000 population\n 4  2023       3 10-14 years Non-Hispanic White      0.1 per 1,000 population\n 5  2023       3 15-19 years All races and origins  13.4 per 1,000 population\n 6  2023       3 15-19 years Hispanic               21.3 per 1,000 population\n 7  2023       3 15-19 years Non-Hispanic Black     19.7 per 1,000 population\n 8  2023       3 15-19 years Non-Hispanic White      8.7 per 1,000 population\n 9  2023       3 20-24 years All races and origins  56   per 1,000 population\n10  2023       3 20-24 years Hispanic               78.3 per 1,000 population\n# ℹ 342 more rows\n\n\nSo, for our final dataset, we have 352 observations and 6 variables! However, in our exporatory analysis, we are going to look at the average of the rates over the entire 11 quarter time period, so we will drop the variables “Year”, “Quarter”, and “Unit” since they are not relevant for this analysis. All units are the same (per 1,000 population). This gives us 3 variables (Age_Years, Race_Ethnicity, and Rate).\n\n#Remove the \"Year\", \"Quarter\", \"Unit\" columns\ncdcdata2 &lt;- cdcdata2 %&gt;%\n  select(-Year, -Quarter, -Unit)\n\n#Exploratory/Descriptive Analysis of Clean Data\nNow let’s begin to explore the data. We can’t show the percentage of observations in each category because we only have rate and not the population size. We also can’t summarize the variables in a way that can be described in a distribution because we don’t have the population size, just the final rates for each group.\nWe can ask: within each Race_Ethnicity category (All races and origins, Hispanic, Non-Hispanic Black, and Non-Hispanic White), how does the birth rate change across age groups during this 11-quarter period?\nFirst, let’s summarize the data so that we get the mean and standard deviation for each age group according to race_ethnicity.\n\n#Collapse data by Age_Years and Race_Ethnicity\ncdcdata2_agg &lt;- cdcdata2 %&gt;%\n  group_by(Age_Years, Race_Ethnicity) %&gt;%\n  summarise_all(list(mean = mean, sd = sd)) %&gt;%\n  ungroup()\n\n#Make a  table using knitr\nknitr::kable(cdcdata2_agg, caption = \"Mean and Standard Deviation of Birth Rates by Age\")\n\n\nMean and Standard Deviation of Birth Rates by Age\n\n\nAge_Years\nRace_Ethnicity\nmean\nsd\n\n\n\n\n10-14 years\nAll races and origins\n0.2000000\n0.0000000\n\n\n10-14 years\nHispanic\n0.3000000\n0.0000000\n\n\n10-14 years\nNon-Hispanic Black\n0.3545455\n0.0522233\n\n\n10-14 years\nNon-Hispanic White\n0.1000000\n0.0000000\n\n\n15-19 years\nAll races and origins\n13.8181818\n0.3789939\n\n\n15-19 years\nHispanic\n21.3000000\n0.3193744\n\n\n15-19 years\nNon-Hispanic Black\n21.1636364\n1.1595454\n\n\n15-19 years\nNon-Hispanic White\n9.2181818\n0.3341203\n\n\n20-24 years\nAll races and origins\n60.2818182\n1.9898835\n\n\n20-24 years\nHispanic\n81.8909091\n1.8019182\n\n\n20-24 years\nNon-Hispanic Black\n76.3454545\n4.4362966\n\n\n20-24 years\nNon-Hispanic White\n50.6272727\n1.8363501\n\n\n25-29 years\nAll races and origins\n92.6090909\n1.1076593\n\n\n25-29 years\nHispanic\n110.3090909\n3.8386077\n\n\n25-29 years\nNon-Hispanic Black\n90.9818182\n1.5879661\n\n\n25-29 years\nNon-Hispanic White\n90.5363636\n0.9254974\n\n\n30-34 years\nAll races and origins\n96.4363636\n1.3086426\n\n\n30-34 years\nHispanic\n99.2727273\n3.9678939\n\n\n30-34 years\nNon-Hispanic Black\n79.1909091\n0.7879778\n\n\n30-34 years\nNon-Hispanic White\n99.9000000\n1.4303846\n\n\n35-39 years\nAll races and origins\n53.8363636\n1.5711605\n\n\n35-39 years\nHispanic\n56.8000000\n2.8722813\n\n\n35-39 years\nNon-Hispanic Black\n49.0000000\n1.5244671\n\n\n35-39 years\nNon-Hispanic White\n52.0727273\n1.1816014\n\n\n40-44 years\nAll races and origins\n12.1909091\n0.4678772\n\n\n40-44 years\nHispanic\n14.8636364\n0.7500303\n\n\n40-44 years\nNon-Hispanic Black\n13.2454545\n0.6486349\n\n\n40-44 years\nNon-Hispanic White\n10.4454545\n0.2769969\n\n\n45+ years\nAll races and origins\n1.0090909\n0.0943880\n\n\n45+ years\nHispanic\n1.0636364\n0.1206045\n\n\n45+ years\nNon-Hispanic Black\n1.2909091\n0.1445998\n\n\n45+ years\nNon-Hispanic White\n0.8363636\n0.0809040\n\n\n\n\nprint(cdcdata2_agg)\n\n# A tibble: 32 × 4\n   Age_Years   Race_Ethnicity          mean     sd\n   &lt;fct&gt;       &lt;fct&gt;                  &lt;dbl&gt;  &lt;dbl&gt;\n 1 10-14 years All races and origins  0.2   0     \n 2 10-14 years Hispanic               0.3   0     \n 3 10-14 years Non-Hispanic Black     0.355 0.0522\n 4 10-14 years Non-Hispanic White     0.1   0     \n 5 15-19 years All races and origins 13.8   0.379 \n 6 15-19 years Hispanic              21.3   0.319 \n 7 15-19 years Non-Hispanic Black    21.2   1.16  \n 8 15-19 years Non-Hispanic White     9.22  0.334 \n 9 20-24 years All races and origins 60.3   1.99  \n10 20-24 years Hispanic              81.9   1.80  \n# ℹ 22 more rows\n\n\nThese are all stable measures because the SDs are small.\nFor each race/ethnic group, I will plot the mean rate for each age group. I am not plotting the rate for each age group and race/ethnic group over time because that would result in 32 lines, and that would be a very messy figure. Instead, I will plot the mean rate for each age group for each race/ethnic group for the entire period.\n\n#ChatGPT helped me with this section.\n\n#Plotting a bar graph\np1 &lt;- ggplot(cdcdata2_agg, aes(x = Age_Years, y = mean, fill = Race_Ethnicity)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Mean Rate by Age Group and Race/Ethnicity\",\n       x = \"Age Group\",\n       y = \"Mean Rate\",\n       fill = \"Race/Ethnicity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_fill_brewer(palette = \"Set2\")  # Choose a color palette if needed\nplot(p1)\n\n\n\n\n\n\n\n#Saving figure\nfigure_file &lt;- here(\"cdcdata-exercise\", \"mean_rate_bar.png\")\nggsave(filename = figure_file, plot=p1)\n\nSaving 7 x 5 in image\n\n#Plotting a line graph\np2 &lt;- ggplot(cdcdata2_agg, aes(x = Age_Years, y = mean, color = Race_Ethnicity, group = Race_Ethnicity)) +\n  geom_line() +\n  labs(title = \"Mean Rate by Age Group and Race/Ethnicity\",\n       x = \"Age Group\",\n       y = \"Mean Rate\",\n       color = \"Race/Ethnicity\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_color_brewer(palette = \"Set2\")  # Choose a color palette if needed\nplot(p2)\n\n\n\n\n\n\n\n#Saving figure\nfigure_file &lt;- here(\"cdcdata-exercise\", \"mean_rate_line.png\")\nggsave(filename = figure_file, plot=p2)\n\nSaving 7 x 5 in image\n\n\nThese graphs tell us how birth rates change as women get older among different race_ethnicity categories. Unsurprisingly, despite differences among racial_ethnic categories, the overall trend remains the same. Birth rates are highest among women in the 25-29 and 30-34 age groups."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Professional Photo\n\n\nI received my PhD in Latin American History with a Concentration in Gender Studies from the University of California, Los Angeles (UCLA) in 2016. I study the history of reproductive health in Latin America, in particular Brazil. In 2021, I decided to start an MPH in Epidemiology to learn data analysis skills that I could employ in my study of the historical roots to current trends in cesarean section rates in Brazil.\nHistorians use data too! I teach on the history of medicine and slavery. See how historians use data to visualize the transatlantic slave trade with this 2-minute video from Slate: The Atlantic Slave Trade in Two Minutes\nI have experience with Stata and SAS, as well as some experience with R and RStudio. I have statistical knowledge in linear and logistic regressions as well as epidemiological measures of association.\nI would like to come out of this class with an intermediate working knowledge of R and RStudio, as I hope to run all future statistical tests in R. After completing the activities for this first week, I also realize how much I want to learn about streamlined workflows and best practices in data management, including best naming practices.\nQuirky fact about me: I love digging up invasive species in my yard. Other than that, I enjoy running and hiking with my dog. I also do aerial silks and aerial rope at Canopy in Athens."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "In this first section I am making sure the main package is installed by trying to refresh my memory on how to do that in R.\n#Getting started\n\n#Check to see that dslabs package is installed\npackage_name &lt;- \"dslabs\"\n\n#Check if the package is in the list of installed packages\nis_installed &lt;- package_name %in% rownames(installed.packages())\n\n#Print the result\nprint(is_installed)\n\n[1] TRUE\n\n#In this chunk of code I used ChatGPT to help with the installed.packages command. I am still very new to R.\nIn this second section, I am loading the appropriate packages and checking the data.\n#Loading and checking data\n\n#Choosing a CRAN mirror URL based on your location\ncran_mirror &lt;- \"https://cloud.r-project.org\"\n\n#Setting the chosen CRAN mirror\noptions(repos = c(CRAN = cran_mirror))\n\n# Install the 'renv' package\ninstall.packages(\"renv\")\n\n\nThe downloaded binary packages are in\n    /var/folders/by/xf620qf90gzg976bjrnzd4p40000gn/T//Rtmpzz1QxC/downloaded_packages\n\n# Load the 'renv' package\nlibrary(renv)\n\n\nAttaching package: 'renv'\n\n\nThe following objects are masked from 'package:stats':\n\n    embed, update\n\n\nThe following objects are masked from 'package:utils':\n\n    history, upgrade\n\n\nThe following objects are masked from 'package:base':\n\n    autoload, load, remove\n\n#Installing renv to help keep track of packages\ninstall.packages(\"renv\")\n\n\nThe downloaded binary packages are in\n    /var/folders/by/xf620qf90gzg976bjrnzd4p40000gn/T//Rtmpzz1QxC/downloaded_packages\n\n#Loading dslabs and dplyr packages\nlibrary(dslabs)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/cassiaroth/Documents/GitHub/MADARoth/cassiaroth-MADA-portfolio\n\n#Looking at help file for gapminder data\nhelp(gapminder)\n\n#Getting overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#Getting summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#Determining type of object\nclass(gapminder)\n\n[1] \"data.frame\"\nIn this third section, I am learning to create new objects.\n#Processing data\n\n#Writing code that assigns only African countries to a new object/variable called africadata\nafricadata &lt;- gapminder %&gt;% \n  filter(continent == \"Africa\")\n\n#Getting overview of data structure for africadata\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n#Getting summary of africadata\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n#In this chunk, I used ChatGPT. I asked it the following question: Using the tidyverse packages in R, how can I write code that assigns only the African countries to a new object/variable called africadata. The name of my data.frame is gapminder.\n\n#Writing code that only contains infant_mortality and life_expectancy within africadata\ninfant_life &lt;- africadata %&gt;% select(infant_mortality, life_expectancy)\n\n#Getting overview of data structure for infant_life\nstr(infant_life)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\n#Getting summary of infant_life\nsummary(infant_life)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n#Writing code that only contains population and life_expectancy within africadata\npopulation_life &lt;- africadata %&gt;% select(population, life_expectancy)\n\n#Getting overview of data structure for population_life\nstr(population_life)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n#Getting summary of population_life\nsummary(population_life)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n#For this chunk of code, I first incorrectly used the filter() command, and I had to troubleshoot my code with ChatGPT, which pointed me to the select() command.\nNow I will plot life expectancy as a function of infant mortality and as a function of population size.\n#Plotting\n\n#Plotting life expectancy as a function of infant mortality\nplot_1 &lt;- ggplot(infant_life, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(alpha = 2.0) +\n  labs(title = \"Life Expectancy as a Function of Infant Mortality\",\n       x = \"Infant Mortality\",\n       y = \"Life Expectancy\")\n\n#Displaying\nprint(plot_1)\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n#Saving as a figure\nfigure_file = here(\"coding-exercise\",\"results\", \"figures\",\"life_exp_inf_mort_all.png\")\nggsave(filename = figure_file, plot=plot_1)\n\nSaving 7 x 5 in image\n\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n#I used ChatGPT with the following command: Using the variables infant_life and population_life, plot life expectancy as a function of infant mortality. I changed the alpha level from 0.7 to 2.0, but I'm not sure if that actually did anything\n\n#Plotting life expectancy as a function of population size on the log scale\nplot_2 &lt;- ggplot(population_life, aes(x = population, y = life_expectancy)) +\n  geom_point(alpha = 2.0) +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy as a Function of Population Size\",\n       x = \"Population Size\",\n       y = \"Life Expectancy\")\n\n#Displaying\nprint(plot_2)\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n#Saving as a figure\nfigure_file = here(\"coding-exercise\",\"results\", \"figures\",\"life_exp_pop_all.png\")\nggsave(filename = figure_file, plot=plot_2) \n\nSaving 7 x 5 in image\n\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\nIt appears there is clustering of data by country and year, since the original dataset had information between 1960 and 2016.\n#More data processing\n\n#Identifying years with missing data for infant mortality\nmissing_years &lt;- unique(africadata[is.na(africadata$infant_mortality), \"year\"])\n\n#Excluding missing years (up to 1981 and 2016) and select data for the year 2000\ndata_2000 &lt;- subset(africadata, !(year %in% c(missing_years, 1981, 2016)) & year == 2000)\n\n#Checking the structure and summary\nstr(data_2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(data_2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n#In this chunk, I used ChatGPT to help with my coding. I used the following prompt: Write code that figures out which years have missing data for infant mortality. Create a new object by extracting only the data for the year 2000 from the africadata object. You should end up with 51 observations and 9 variables. Check it with str and summary.\nHere, we will plot infant mortality for the year 2000. Based on these plots, we still see a negative correlation between infant mortality and life expectancy. There is no noticeable correlation between poulation size and life expectancy.\n#More plotting\n\n#Plotting life expectancy as a function of infant mortality for the year 2000\nplot_3 &lt;- ggplot(data_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(alpha = 2.0) +\n  labs(title = \"Life Expectancy as a Function of Infant Mortality, 2000\",\n       x = \"Infant Mortality\",\n       y = \"Life Expectancy\")\n\n#Displaying\nprint(plot_3)\n\n\n\n\n\n\n\n#Saving as a figure\nfigure_file = here(\"coding-exercise\",\"results\", \"figures\",\"life_exp_inf_mort_2000.png\")\nggsave(filename = figure_file, plot=plot_3)\n\nSaving 7 x 5 in image\n\n#Plotting life expectancy as a function of population size for the year 2000\nplot_4 &lt;- ggplot(data_2000, aes(x = population, y = life_expectancy)) +\n  geom_point(alpha = 2.0) +\n  scale_x_log10() +\n  labs(title = \"Life Expectancy as a Function of Population Size, 2000\",\n       x = \"Population Size\",\n       y = \"Life Expectancy\")\n\n#Displaying\nprint(plot_4)\n\n\n\n\n\n\n\n#Saving as a figure\nfigure_file = here(\"coding-exercise\",\"results\", \"figures\",\"life_exp_pop_2000.png\")\nggsave(filename = figure_file, plot=plot_4) \n\nSaving 7 x 5 in image\nNow, we will us a linear model to fit the relationship between life expectancy (outcome) and infant mortality (predictor).\n#Simple model fits1\n\n#Fitting linear regression model for life expectancy (outcome) and infant mortality (predictor)\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = data_2000)\n\n#Printing summary of the regression model\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = data_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n#Simple model fit2\n\n#Fitting linear regression model for life expectancy (outcome) and population (predictor)\nfit2 &lt;- lm(life_expectancy ~ population, data = data_2000)\n\n#Printing summary of the regression model\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = data_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n#For this chunk of code, I asked ChatGPT the following:Use the lm function and fit life expectancy as the outcome, and infant mortality as the predictor. Use the data_2000 dataframe.\nBased on the p-values for {fit1} of p = 2 x 10^-8 &lt; p = 0.05, we can reject the null hypothesis for our first model, and conclude that life expectancy is correlated with infant mortality rates. For {fit2} of p = 0.6159 &gt; p = 0.05, we fail to reject the null hypothesis that life expectancy is not correlated with population size."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#loading-and-checking-the-data",
    "href": "coding-exercise/coding-exercise.html#loading-and-checking-the-data",
    "title": "R Coding Exercise",
    "section": "Loading and checking the data",
    "text": "Loading and checking the data\nThe following code chunk loads the stars dataset from the dslabs package and prints its structure and contents.\nNote that I tend to “comment out” the command to install a package. This is to ensure that I don’t always install the package when I load the libraries, because (other than the annoying case of updating R) you should only have to install a package once on a local device. In any new session, however, you need to load the required libraries.\n\n## installing and loading required packages \n\n#install.packages(\"dslabs\") #install dslabs package \nlibrary(dslabs) #load dslabs package\n\n## load in `stars` dataset from `dslabs` package and view its structure and contents\n# load the dataset into object `stars`\nstars = dslabs::stars #loads dataset from dslabs package\n\n# what type of object is `stars`?\nclass(stars) #checks the class of `stars`\n\n[1] \"data.frame\"\n\n# what is the structure of the data frame `stars`\nstr(stars)\n\n'data.frame':   96 obs. of  4 variables:\n $ star     : Factor w/ 95 levels \"*40EridaniA\",..: 87 85 48 38 33 92 49 79 77 47 ...\n $ magnitude: num  4.8 1.4 -3.1 -0.4 4.3 0.5 -0.6 -7.2 2.6 -5.7 ...\n $ temp     : int  5840 9620 7400 4590 5840 9900 5150 12140 6580 3200 ...\n $ type     : chr  \"G\" \"A\" \"F\" \"K\" ...\n\n# print a summary of the variables in the data frame `stars`\nsummary(stars)\n\n          star      magnitude           temp           type          \n Altair     : 2   Min.   :-8.000   Min.   : 2500   Length:96         \n *40EridaniA: 1   1st Qu.:-1.800   1st Qu.: 3168   Class :character  \n *40EridaniB: 1   Median : 2.400   Median : 5050   Mode  :character  \n *40EridaniC: 1   Mean   : 4.257   Mean   : 8752                     \n *61CygniA  : 1   3rd Qu.:11.325   3rd Qu.: 9900                     \n *61CygniB  : 1   Max.   :17.000   Max.   :33600                     \n (Other)    :89                                                      \n\n\nstars is a data frame with 96 observations and 4 variables. Each observation is a star, and the name of the star is given by the factor variable star. For each observation of each star, the size, magnitude; temperature, temp; and type, type, are listed as variables.\nLet’s clean the data a little. I noticed three potential bugs."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#cleaning-the-data",
    "href": "coding-exercise/coding-exercise.html#cleaning-the-data",
    "title": "R Coding Exercise",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nFirst, there are 96 observations, but only 95 levels of the factor variable star. This means that physical information on 95 different stars is available, but one star has two observations. This could mean that there are two significantly different entries for this star, although there is no “time” variable indicating when the different observations were taken; more likely, this means that there are two different stars whose names were incorrectly entered the same, or that one observation was duplicated.\nSecond, the variable temp has character values, but elements should be numeric. That’s an easy enough fix with the function as.numeric()!\nLastly, the type variable seems to be categorical, with multiple observations of the same type. However, the variable is not factored in the dataframe. We will need to replace the type variable with a factored version of it.\nThe following code chunk will accomplish 3 things:\n\nCheck to see which star has multiple observations, and whether the observations are duplicates.\nChange the temp variable from class chr to class num. This way, R will recognize these values as numberic, rather than character.\nFactor the type variable.\n\n\n## which star has two entries? \nstars %&gt;% \n  dplyr::count(star)  %&gt;% #create a dataframe with variable `star` as first column, listing the levels of factor variable `star`, and number of observations with that factor as the second column, `n`\n  dplyr::filter(n &gt; 1) # filter this new dataframe to only show row where the value of variable `n` is greater than 1. This will print the name of the factor level of `star` with more than one observation in the dataframe `stars`\n\n    star n\n1 Altair 2\n\n## what do these observations look like?\nstars %&gt;% \n  dplyr::filter(star == \"Altair\")\n\n    star magnitude temp type\n1 Altair       2.2 8060    A\n2 Altair       2.2 8060    A\n\n## clean the dataset and save as a new data frame, `stars_cleaned`\nstars_cleaned = stars %&gt;% \n  dplyr::filter(star != \"Altair\") %&gt;% #remove entries for star with two observations\n  dplyr::mutate(temp = as.numeric(temp)) %&gt;% #coerce character variable `temp` to class numeric\n  dplyr::mutate(type = factor(type))\n\n## double check these worked! \nstr(stars_cleaned)\n\n'data.frame':   94 obs. of  4 variables:\n $ star     : Factor w/ 95 levels \"*40EridaniA\",..: 87 85 48 38 33 92 49 79 77 47 ...\n $ magnitude: num  4.8 1.4 -3.1 -0.4 4.3 0.5 -0.6 -7.2 2.6 -5.7 ...\n $ temp     : num  5840 9620 7400 4590 5840 ...\n $ type     : Factor w/ 10 levels \"A\",\"B\",\"DA\",\"DB\",..: 7 1 6 8 7 1 7 2 6 9 ...\n\n\nI used the count() and filter() functions within the dplyr package to identify the level of factor star with two observations. It turns out that the star “Altair” was the factor with two observations, and the observations are duplicates. Whether these observations were taken at different times or are the result of a naming misentry, and thus contain different information, is unknown; so, I chose to remove these observations.\nI saved the cleaned dataframe, with observations for the star “Altair” removed and the temp variable changed from class chr to class num, to the data frame stars_cleaned."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-the-data",
    "href": "coding-exercise/coding-exercise.html#processing-the-data",
    "title": "R Coding Exercise",
    "section": "Processing the data",
    "text": "Processing the data\nstars_cleaned is a data frame with 94 observations and 4 variables. Each observation is a star, and the name of the star is given by the factor variable star. For each observation of each star, the size, magnitude; temperature, temp; and type, type, are listed as variables.\nThe following code chunk creates two box plots: one describing magnitude by each type, and one describing temp by each type.\n\n##plotting \n# boxplot of magnitude by type\nplot1 = ggplot() + geom_boxplot(data = stars_cleaned, aes(x = type, y = magnitude, col = type))\n\n# boxpot of temperature by type\nplot2 = ggplot() + geom_boxplot(data = stars_cleaned, aes(x = type, y = temp))\n\nI’m saving those plots in this code chunk so that I get in the habit now! They will be located in the “results” folder, within the “coding-exercise” folder.\n\n# save plot 1\nfigure_file = here(\"coding-exercise\",\"results\", \"figures\",\"mag_by_type_box.png\") # file name and location\nggsave(filename = figure_file, plot=plot1) # save plot 1 to that file name and location\n\nSaving 7 x 5 in image\n\n# save plot 2\nfigure_file = here(\"coding-exercise\",\"results\", \"figures\", \"temp_by_type_box.png\")\nggsave(filename = figure_file, plot=plot2) \n\nSaving 7 x 5 in image\n\n\nAnd now I am loading them as Figure 1 and Figure 2!\n\n\n\n\n\n\n\n\nFigure 1: Summary of magnitude distributions by star type.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Summary of temperature distributions by star type.\n\n\n\n\n\nI notice that some of the types show little variation in temperature and magnitude. I want to check to see how even our observations of different types are. So, I am going to create a table which shows the number of observations recorded for each factor level of type.\n\n#generate a table of the number of observations of each type of star\nfreqTable.by.type = t(as.matrix(table(stars_cleaned$type))) #generate table of the number of observations of each type with table(), transfrom it into a 10x1 matrix, and transpose into a 1x10 matrix\ntable1 = as.data.frame(freqTable.by.type) #convert from matrix to dataframe for ease of naming and saving table\nrownames(table1) = c(\"Number of Observations\") #name the frequency row for presentation\n\nThe following code chunk saves the table of counts of each type of star:\n\n# save count tables\nsummarytable_file = here(\"coding-exercise\",\"results\", \"tables-files\", \"type_count_table.rds\")\nsaveRDS(table1, file = summarytable_file)\n\nAnd this code chunk loads it as Table 1!\n\n\n\n\nTable 1: Number of observations of each type of star.\n\n\n\n\n\n\n\nA\nB\nDA\nDB\nDF\nF\nG\nK\nM\nO\n\n\n\n\nNumber of Observations\n11\n19\n2\n1\n1\n7\n4\n16\n32\n1\n\n\n\n\n\n\n\n\nClearly, there are very few observations of stars of type “DA”, “DB”, “DF”, “G”, and “O”. I would like to look at the trends in temp and magnitude based on type, so I only want to consider types with more than a certain number of observations."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-processing",
    "href": "coding-exercise/coding-exercise.html#more-processing",
    "title": "R Coding Exercise",
    "section": "More Processing",
    "text": "More Processing\nFor the sake of this activity, I have required that each type represented in my dataset have greater than or equal to 5 observations.\nThe code chunk below filters out observations by the types which have fewer than 5 observations.\n\n#remove stars of type with fewer than 5 observations\nstars_cleaned_2 = stars_cleaned %&gt;%\n  group_by(type) %&gt;% #group by type\n  filter(n() &gt;= 5) #filter for type with greater than or equal to 5 observations\n\n#lets double check that all remaining types have grater than 7 observations\nstars_cleaned_2 %&gt;% count(type)\n\n# A tibble: 5 × 2\n# Groups:   type [5]\n  type      n\n  &lt;fct&gt; &lt;int&gt;\n1 A        11\n2 B        19\n3 F         7\n4 K        16\n5 M        32"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-plotting",
    "href": "coding-exercise/coding-exercise.html#more-plotting",
    "title": "R Coding Exercise",
    "section": "More plotting",
    "text": "More plotting\nNow, I would like to investigate 3 things:\n\nHow does temperature correlate with magnitude?\nDo these correlations depend on type?\nWhat is the nature of these correlations? Are they significiant?\n\n\n#scatterplot of magnitude by temperature\nplot3 = ggplot() + geom_point(data = stars_cleaned_2, aes(x = magnitude, y = temp)) + labs(title = \"Association of star temperatures with their magnitude\")\n\n#scatterplot of magnitude by temperature on log scale\nplot4 = ggplot() + geom_point(data = stars_cleaned_2, aes(x = magnitude, y = temp)) + \n  scale_y_continuous(trans = \"log\") +\n  labs(title = \"Association of star temperatures with their magnitude\")\n\n#save plots to figures file \nfigure_file = here(\"coding-exercise\",\"results\", \"figures\", \"magnitude-temp-scatter.png\")\nggsave(filename = figure_file, plot=plot3) \n\nSaving 7 x 5 in image\n\nfigure_file = here(\"coding-exercise\",\"results\", \"figures\", \"magnitude-logtemp-scatter.png\")\nggsave(filename = figure_file, plot=plot4) \n\nSaving 7 x 5 in image\n\n\n\n\n\n\n\n\n\n\nFigure 3: Scatterplot of star magnitudes by temperature.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Scatterplot of star magnitudes by temperature (log scale).\n\n\n\n\n\nThere appears to be a negative association between the temperature of a star and its size (Figure 3), and this relationship appears more linear on a log-linear plot (Figure 4).\nGrouping by type in Figure 5, we see that stars of the same type tend to group together according to the variable temp, and to a lesser extent, also by the variable magnitude. This is somewhat reflected in our boxplots (Figure 1 and Figure 2.)\n\n#scatterplot of magnitude by temperature on log scale\nplot6 = ggplot() + geom_point(data = stars_cleaned_2, aes(x = magnitude, y = temp, col=type)) + \n  scale_y_continuous(trans = \"log\") +\n  labs(title = \"Association of star temperatures with their magnitude\")\n\n#save plots to figures file \nfigure_file = here(\"coding-exercise\",\"results\", \"figures\", \"grouped-magnitude-logtemp-scatter.png\")\nggsave(filename = figure_file, plot=plot6) \n\nSaving 7 x 5 in image\n\n\n\n\n\n\n\n\n\n\nFigure 5: Scatterplot of star magnitudes by temperature (log scale), grouped by star type."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#regression-analysis",
    "href": "coding-exercise/coding-exercise.html#regression-analysis",
    "title": "R Coding Exercise",
    "section": "Regression analysis",
    "text": "Regression analysis\nBased on these plots, I would like to examine three things:\n\nDisregarding star type, is there a strong linear association between temperature and magnitude?\nDisregarding star type, is the log-linear association between temp~magnitude stronger?\nAre the within-type regressions even stronger fits?\n\nNote on part (3) that the smaller within-group sample sizes will reduce the power of the fit. But for simple exploration, let’s set this consideration aside.\nFirst, let’s examine the linear model relating the magnitude of all stars with their temp on a linear scale.\nThe following code chunks find the best-fit linear model, save the model summary to a table, and load the table.\n\n## saving linear regression model tables will require the flextable and gtsummary packages\n\n# install.packages(\"flextable\") # if you don't have this package already installed, uncomment and run this line\nlibrary(flextable) #load flextable library\n# install.packages(\"gtsummary\") # if you don't have this package already installed, uncomment and run this line\nlibrary(gtsummary) #load gtsummary library\n\n\nAttaching package: 'gtsummary'\n\n\nThe following objects are masked from 'package:flextable':\n\n    as_flextable, continuous_summary\n\n## Linear Model 1 - temp and magnitude, ungrouped, linear scale\n#linear regression, fixed effect, temp ~ magnitude\nlm.1 = lm(data = stars_cleaned_2, temp ~ magnitude)\nsummary(lm.1) # what does our model look like? \n\n\nCall:\nlm(formula = temp ~ magnitude, data = stars_cleaned_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12075.9  -2385.0   -722.4   1638.8  13905.4 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11315.18     700.60  16.151  &lt; 2e-16 ***\nmagnitude    -694.86      82.43  -8.429 9.05e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5668 on 83 degrees of freedom\nMultiple R-squared:  0.4612,    Adjusted R-squared:  0.4547 \nF-statistic: 71.06 on 1 and 83 DF,  p-value: 9.045e-13\n\ntable1 = lm.1 %&gt;% gtsummary::tbl_regression() #save summaries as tables using tbl_regression() function from gtsummary package\n\n\n# save summary tables\nsummarytable_file = here(\"coding-exercise\",\"results\", \"tables-files\", \"linear_model_temp_mag_ungrouped.rds\")\nsaveRDS(table1, file = summarytable_file)\n\n#extract p-value and save for plot\np.mag.linear = unname(summary(lm.1)$coefficients[,\"Pr(&gt;|t|)\"])[2]\n\n\n\n\n\nTable 2: Linear model of star temperature predicted by magnitude.\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nmagnitude\n-695\n-859, -531\n&lt;0.001\n\n\n\n\n\n\n\n\nThe following code chunks plot the linear model against our observations, and load the figure.\n\n#scatterplot of magnitude by temperature\nplot7 = ggplot() + geom_point(data = stars_cleaned_2, aes(x = magnitude, y = temp)) + \n  geom_smooth(data = stars_cleaned_2, aes(y = temp, x = magnitude), method = 'lm') +\n   annotate(geom = \"text\", x = 10, y = 25000, label = paste(\"p-value =\", signif(p.mag.linear, digits = 3)), color = \"maroon\") +\n  labs(title = \"Association of star temperatures with their magnitude\")\n\n#save plots to figures file \nfigure_file = here(\"coding-exercise\",\"results\", \"figures\", \"magnitude-temp-scatter-lm.png\")\nggsave(filename = figure_file, plot=plot7) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFigure 6: Scatterplot of star magnitudes by temperature (log scale).\n\n\n\n\n\nAccording to Table 2, a linear model of temperature predicted by magnitude has a significant fit, that is, \\(p&lt;0.05\\). However, Figure 6 demonstrates that there is a clear curve in the relationship that may be somewhat attenuated by plotting temperature on a log scale (Figure 4).\nLet’s examine the regression on a log-linear scale.\n\n## saving linear regression model tables will require the flextable and gtsummary packages\n\n# install.packages(\"flextable\") # if you don't have this package already installed, uncomment and run this line\nlibrary(flextable) #load flextable library\n# install.packages(\"gtsummary\") # if you don't have this package already installed, uncomment and run this line\nlibrary(gtsummary) #load gtsummary library\n\n## Linear Model 1 - temp and magnitude, ungrouped, linear scale\n#linear regression, fixed effect, temp ~ magnitude\nlm.2 = lm(data = stars_cleaned_2, log(temp) ~ magnitude)\nsummary(lm.2) # what does our model look like? \n\n\nCall:\nlm(formula = log(temp) ~ magnitude, data = stars_cleaned_2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.46269 -0.13691 -0.01977  0.22370  0.84892 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  9.055630   0.057815  156.63   &lt;2e-16 ***\nmagnitude   -0.083854   0.006802  -12.33   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4677 on 83 degrees of freedom\nMultiple R-squared:  0.6467,    Adjusted R-squared:  0.6425 \nF-statistic:   152 on 1 and 83 DF,  p-value: &lt; 2.2e-16\n\ntable2 = lm.2 %&gt;% gtsummary::tbl_regression() #save summaries as tables using tbl_regression() function from gtsummary package\n\n\n# save summary tables\nsummarytable_file = here(\"coding-exercise\",\"results\", \"tables-files\", \"log_linear_model_temp_mag_ungrouped.rds\")\nsaveRDS(table1, file = summarytable_file)\n\n#extract p-value and save for plot\np.mag.loglinear = unname(summary(lm.2)$coefficients[,\"Pr(&gt;|t|)\"])[2]\n\n\n\n\n\nTable 3: Linear model of star log(temperature) predicted by magnitude.\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI\np-value\n\n\n\n\nmagnitude\n-695\n-859, -531\n&lt;0.001\n\n\n\n\n\n\n\n\n\n#scatterplot of magnitude by temperature\nplot8 = ggplot() + geom_point(data = stars_cleaned_2, aes(x = magnitude, y = log(temp))) + \n  geom_smooth(data = stars_cleaned_2, aes(y = log(temp), x = magnitude), method = 'lm') +\n   annotate(geom = \"text\", x = 10, y = 9.5, label = paste(\"p-value =\", signif(p.mag.loglinear, digits = 3)), color = \"forestgreen\") +\n  labs(title = \"Association of star temperatures with their magnitude\", y = \"log(temp)\")\n\n#save plots to figures file \nfigure_file = here(\"coding-exercise\",\"results\", \"figures\", \"magnitude-logtemp-scatter-lm.png\")\nggsave(filename = figure_file, plot=plot8) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFigure 7: Scatterplot of temperature (log scale) by magnitude.\n\n\n\n\n\nAccording to Table 3, a linear model of log(temperature) predicted by magnitude has a significant fit, that is, \\(p&lt;0.05\\), just like its linear-linear precursor. However, this p-value is multiple orders of magnitude smaller, and Figure 7 demonstrates that the curve in the temperature-magnitude association is significantly reduced by plotting temperature on a log scale.\nWithout running our regression models, lets see what our regressions look like by type.\n\n#scatterplot of magnitude by temperature on log scale\nplot9 = ggplot() + geom_point(data = stars_cleaned_2, aes(x = magnitude, y = temp, col=type)) + \n  scale_y_continuous(trans = \"log\") +\n   geom_smooth(data = stars_cleaned_2, aes(y = temp, x = magnitude, col = type), method = 'lm') +\n  labs(title = \"Association of star temperatures with their magnitude\")\n\n#save plots to figures file \nfigure_file = here(\"coding-exercise\",\"results\", \"figures\", \"grouped-magnitude-logtemp-scatter-lm.png\")\nggsave(filename = figure_file, plot=plot9) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFigure 8: Scatterplot of star magnitudes by temperature (log scale), grouped by star type.\n\n\n\n\n\nFigure 8 reveals that the associations between magnitude and log(temperature) are different across star types, some demonstrating strong negative correlations (like type “B”), and some with more neutral correlations (like type “F”)."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Complex Data Exercise",
    "section": "",
    "text": "This exercise is designed to give you practice with exploring text datasets (a complex data type). We will load, process, and engage in some discriptive/exploratory analysis of the data.\nThis dataset, titled Sherlock, is a package that includes text from the Sherlock Holmes book series by Sir Arthur Conan Doyle. All 48 texts are in the public domain. Information regarding copyright laws is available here.\nI found this dataset through Emil Hvitfeldt’s R-text-data compilation repository. I also am using Paul Vanderlaken’s website as guidance.\nFirst, we will install and load the dataset.\n\n#suppressing log messages\n#| message: false\n#| warning: false\n#| include: false\n\n#installing dataset and other needed packages\ndevtools::install_github(\"EmilHvitfeldt/sherlock\")\n\nSkipping install of 'sherlock' from a github remote, the SHA1 (38584034) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n#loading dataset and other necessary packages\nlibrary(sherlock)\nlibrary(tidytext)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.4\n✔ ggplot2   3.4.4     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(stringr)\nlibrary(tibble)\nlibrary(knitr)\nlibrary(here)\n\nhere() starts at /Users/cassiaroth/Documents/GitHub/MADARoth/cassiaroth-MADA-portfolio\n\n\nNext, we will explore the dataset. We will start by looking at the structure.\n\n#Exploring the dataset\ndplyr::glimpse(holmes)\n\nRows: 65,958\nColumns: 2\n$ text &lt;chr&gt; \"A STUDY IN SCARLET\", \"\", \"Table of contents\", \"\", \"Part I\", \"Mr.…\n$ book &lt;chr&gt; \"A Study In Scarlet\", \"A Study In Scarlet\", \"A Study In Scarlet\",…\n\n#Viewing the first few rows (variable names \"text\" and \"book\")\nhead(holmes)\n\n# A tibble: 6 × 2\n  text                  book              \n  &lt;chr&gt;                 &lt;chr&gt;             \n1 \"A STUDY IN SCARLET\"  A Study In Scarlet\n2 \"\"                    A Study In Scarlet\n3 \"Table of contents\"   A Study In Scarlet\n4 \"\"                    A Study In Scarlet\n5 \"Part I\"              A Study In Scarlet\n6 \"Mr. Sherlock Holmes\" A Study In Scarlet\n\n#Understanding all books in the data\nbook_titles &lt;- holmes %&gt;% distinct(book)\nprint(book_titles)\n\n# A tibble: 48 × 1\n   book                               \n   &lt;chr&gt;                              \n 1 A Study In Scarlet                 \n 2 The Sign of the Four               \n 3 A Scandal in Bohemia               \n 4 The Red-Headed League              \n 5 A Case of Identity                 \n 6 The Boscombe Valley Mystery        \n 7 The Five Orange Pips               \n 8 The Man with the Twisted Lip       \n 9 The Adventure of the Blue Carbuncle\n10 The Adventure of the Speckled Band \n# ℹ 38 more rows\n\n#Ordering the entries alphabetically by book title\nbook_titles &lt;- book_titles[order(book_titles$book), ]\n\n#Creating a table using kable\ntable1 &lt;- kable(book_titles, caption = \"Book Titles\")\nprint(table1)\n\n\n\nTable: Book Titles\n\n|book                                        |\n|:-------------------------------------------|\n|A Case of Identity                          |\n|A Scandal in Bohemia                        |\n|A Study In Scarlet                          |\n|His Last Bow                                |\n|Silver Blaze                                |\n|The \"Gloria Scott\"                          |\n|The Adventure of Black Peter                |\n|The Adventure of Charles Augustus Milverton |\n|The Adventure of the Abbey Grange           |\n|The Adventure of the Beryl Coronet          |\n|The Adventure of the Blue Carbuncle         |\n|The Adventure of the Bruce-Partington Plans |\n|The Adventure of the Cardboard Box          |\n|The Adventure of the Copper Beeches         |\n|The Adventure of the Dancing Men            |\n|The Adventure of the Devil's Foot           |\n|The Adventure of the Dying Detective        |\n|The Adventure of the Empty House            |\n|The Adventure of the Engineer's Thumb       |\n|The Adventure of the Golden Pince-Nez       |\n|The Adventure of the Missing Three-Quarter  |\n|The Adventure of the Noble Bachelor         |\n|The Adventure of the Norwood Builder        |\n|The Adventure of the Priory School          |\n|The Adventure of the Red Circle             |\n|The Adventure of the Second Stain           |\n|The Adventure of the Six Napoleons          |\n|The Adventure of the Solitary Cyclist       |\n|The Adventure of the Speckled Band          |\n|The Adventure of the Three Students         |\n|The Adventure of Wisteria Lodge             |\n|The Boscombe Valley Mystery                 |\n|The Crooked Man                             |\n|The Disappearance of Lady Frances Carfax    |\n|The Final Problem                           |\n|The Five Orange Pips                        |\n|The Greek Interpreter                       |\n|The Hound of the Baskervilles               |\n|The Man with the Twisted Lip                |\n|The Musgrave Ritual                         |\n|The Naval Treaty                            |\n|The Red-Headed League                       |\n|The Reigate Squires                         |\n|The Resident Patient                        |\n|The Sign of the Four                        |\n|The Stock-Broker's Clerk                    |\n|The Valley Of Fear                          |\n|The Yellow Face                             |\n\n\nNow let’s see how many times some of the most important people’s names appears in the texts.\n\n#Searching for specific words\nspecific_words &lt;- c(\"sherlock\", \"holmes\", \"moriarty\", \"watson\", \"john\")\n\n#Creating a word frequency table for specific words\nword_freq_table &lt;- holmes %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(word %in% specific_words) %&gt;%\n  count(word, sort = TRUE)\n\nword_freq_df &lt;- as.data.frame(word_freq_table)\nprint(word_freq_df)\n\n      word    n\n1   holmes 2403\n2   watson  809\n3 sherlock  383\n4     john  118\n5 moriarty   49\n\n\nMistakenly, many people attribute the quote “Elementary, my dear Watson,” to the Sherlock Holmes series. So let’s check how many times the phrase appears in the data. For fun, we will also see if Sherlock Holmes ever calls his sidekick “John Watson” by his full name.\n\n#Searching for phrases about Watson\nphrases &lt;- c(\"John Watson\", \"Elementary, my dear Watson\")\n\nholmes_filtered &lt;- holmes %&gt;%\n  filter(str_detect(text, fixed(phrases[1])) | \n         str_detect(text, fixed(phrases[2])))\n\n#Viewing the filtered dataset\nprint(holmes_filtered)\n\n# A tibble: 2 × 2\n  text                                                     book              \n  &lt;chr&gt;                                                    &lt;chr&gt;             \n1 A Continuation Of The Reminiscences Of John Watson, M.D. A Study In Scarlet\n2 A Continuation Of The Reminiscences Of John Watson, M.D. A Study In Scarlet\n\n\nWe can see that “John Watson” appears only twice, when he is writing about his own experiences. The phrase “Elementary, my dear Watson,” appears zero times. So, it’s a myth that Sherlock Holmes ever said this phrase in the original texts.\nI was going to try to create a word cloud, but I was having trouble loading the package; RStudio kept crashing. Despite this not showing that much information, I’ve spent almost 3 hours on the exercise. I need to get better with R!"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cassia’s Data Analysis Website",
    "section": "",
    "text": "Welcome!\nI am an MPH student in Epidemiology in the College of Public Health at the University of Georgia (UGA). I am also an Associate Professor of History and Latin American and Caribbean Studies in the Franklin College of Arts and Sciences at UGA.\n\nPlease use the Menu Bar above to look around.\n\n\n\n\nMe and my dog Fox at Acadia National Park in Maine"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "Placeholder file for the future data/results presentation exercise."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "Rachel Robertson contributed to this exercise.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nParty\n0\n1\nFALSE\n4\nDem: 5, Lib: 2, Gre: 1, Rep: 1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nAge\n0\n1\nNA\nNA\nNA\n38.88889\n23.22953\n9\n23\n33\n56\n77\n▅▇▂▂▅"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/cassiaroth/Documents/GitHub/MADARoth/cassiaroth-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Party                 0             1 FALSE          4\n  top_counts                    \n1 M: 4, F: 3, O: 2              \n2 Dem: 5, Lib: 2, Gre: 1, Rep: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  38.9 23.2   9  23  33  56   77 ▅▇▂▂▅\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/cassiaroth/Documents/GitHub/MADARoth/cassiaroth-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/cassiaroth/Documents/GitHub/MADARoth/cassiaroth-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                     `Allowed Values`    \n  &lt;chr&gt;           &lt;chr&gt;                                     &lt;chr&gt;               \n1 Height          height in centimeters                     numeric value &gt;0 or…\n2 Weight          weight in kilograms                       numeric value &gt;0 or…\n3 Gender          identified gender (male/female/other)     M/F/O/NA            \n4 Age             Age in years                              numeric value &gt;0 or…\n5 Party           Political party affiliation (categorical) Republican/Democrat…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n$ Age    &lt;dbl&gt; 17, 25, 89, 65, 49, 33, 23, 9, 12, 99, 26, 56, 77, 45\n$ Party  &lt;chr&gt; \"Republican\", \"Democrat\", \"Democrat\", \"Democrat\", \"Republican\",…\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   : 9.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:23.50  \n Mode  :character   Median :  70.0   Mode  :character   Median :39.00  \n                    Mean   : 602.7                      Mean   :44.64  \n                    3rd Qu.:  90.0                      3rd Qu.:62.75  \n                    Max.   :7000.0                      Max.   :99.00  \n                    NA's   :1                                          \n    Party          \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age Party      \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;      \n1 180        80 M         17 Republican \n2 175        70 O         25 Democrat   \n3 sixty      60 F         89 Democrat   \n4 178        76 F         65 Democrat   \n5 192        90 NA        49 Republican \n6 6          55 F         33 Libertarian\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nParty\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.0\n70\n90.00\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n44.64\n29.05\n9\n23.5\n39\n62.75\n99\n▇▂▂▂▂\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nParty\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n41.23\n27.16\n9\n23.00\n33\n56\n99\n▇▂▂▂▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nParty\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n41.23\n27.16\n9\n23.00\n33\n56\n99\n▇▂▂▂▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nParty\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n37.36\n22.62\n9\n20.0\n33\n52.5\n77\n▇▇▅▂▅\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nParty\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179.0\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85.0\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n37.36\n22.62\n9\n20.0\n33\n52.5\n77\n▇▇▅▂▅\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nParty\n0\n1\n5\n11\n0\n4\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n38.89\n23.23\n9\n23\n33\n56\n77\n▅▇▂▂▅\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nd4$Party &lt;- as.factor(d4$Party)  \nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\nParty\n0\n1\nFALSE\n4\nDem: 5, Lib: 2, Gre: 1, Rep: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n38.89\n23.23\n9\n23\n33\n56\n77\n▅▇▂▂▅\n\n\n\n\n\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Cassia Roth's Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  }
]